{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HuJmAsVAJsR"
   },
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W02DJB4g_00X",
    "outputId": "349cb8f6-7561-4a76-e02d-50ed47994033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/323.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h All dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Install all required libraries quietly\n",
    "!pip install ipywidgets transformers torch sentencepiece huggingface_hub pypdf evaluate scikit-learn sentence-transformers matplotlib seaborn pandas nltk textstat rouge_score --quiet\n",
    "# accelerate is needed for efficient multi-device model loading\n",
    "!pip install accelerate --quiet\n",
    "print(\" All dependencies installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNFGjqvnAIzG"
   },
   "source": [
    "Import Libraries & Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jtg8zUTF_40a",
    "outputId": "abfee703-08bd-4080-c7c8-23f562fa91eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face Hub login successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pypdf\n",
    "import evaluate\n",
    "import nltk\n",
    "import io\n",
    "import warnings\n",
    "import time\n",
    "import textstat\n",
    "from math import pi\n",
    "\n",
    "# --- 0. SETUP AND AUTHENTICATION ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# CORRECTED: Download the 'punkt' resource and the specific 'punkt_tab' to fix the TextRank error\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True) # <-- This line explicitly fixes the error\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")\n",
    "\n",
    "# Securely get the token from Colab secrets\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\" Hugging Face Hub login successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face Hub login failed. Please ensure you have set the 'HF_TOKEN' secret correctly.\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6Q0-CUbAPpI"
   },
   "source": [
    " Load AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pw5z1274BJM1",
    "outputId": "6e281752-2680-4267-c601-878dd354e615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Loading small, fast models... This should be quicker now. ⏳\n",
      "Using device: cuda\n",
      "Loaded TinyLlama-1.1B-Chat.\n",
      "Loaded Phi-1.5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded BART-Large-CNN.\n",
      "Loaded Sentence Transformer for TextRank.\n",
      "\n",
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install the evaluate library and rouge_score\n",
    "!pip install evaluate rouge_score --quiet\n",
    "\n",
    "import torch # Import the torch library\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # Import necessary classes from transformers\n",
    "from sentence_transformers import SentenceTransformer # Import SentenceTransformer\n",
    "import evaluate # Import the evaluate library\n",
    "\n",
    "# --- 1. LOAD MODELS (PRE-COMPUTATION) ---\n",
    "print(\"Loading small, fast models... This should be quicker now. ⏳\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model Dictionary to hold all loaded models\n",
    "MODELS = {}\n",
    "\n",
    "try:\n",
    "    # Model 1: TinyLlama-1.1B-Chat (Open, 1.1B)\n",
    "    model_id_tiny = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    MODELS['tinyllama'] = {\n",
    "        'tokenizer': AutoTokenizer.from_pretrained(model_id_tiny),\n",
    "        'model': AutoModelForCausalLM.from_pretrained(\n",
    "            model_id_tiny,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        ),\n",
    "        'name': \"TinyLlama-1.1B-Chat\"\n",
    "    }\n",
    "    print(\"Loaded TinyLlama-1.1B-Chat.\")\n",
    "\n",
    "    # Model 2: Phi-1.5 (Restricted License, 1.3B)\n",
    "    model_id_phi = \"microsoft/phi-1.5\"\n",
    "    MODELS['phi'] = {\n",
    "        'tokenizer': AutoTokenizer.from_pretrained(model_id_phi, trust_remote_code=True),\n",
    "        'model': AutoModelForCausalLM.from_pretrained(\n",
    "            model_id_phi,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        ),\n",
    "        'name': \"Phi-1.5\"\n",
    "    }\n",
    "    print(\"Loaded Phi-1.5.\")\n",
    "\n",
    "    # Model 3: BART (Public Baseline, ~400M)\n",
    "    model_id_bart = \"facebook/bart-large-cnn\"\n",
    "    MODELS['bart'] = {\n",
    "        'summarizer': pipeline(\"summarization\", model=model_id_bart, device=0 if device==\"cuda\" else -1),\n",
    "        'name': 'BART-Large-CNN'\n",
    "    }\n",
    "    print(\" Loaded BART-Large-CNN.\")\n",
    "\n",
    "    # Model 4: Sentence Transformer (for Extractive Baseline)\n",
    "    MODELS['embedding'] = {\n",
    "        'model': SentenceTransformer('all-MiniLM-L6-v2', device=device),\n",
    "        'name': 'TextRank (Embeddings)'\n",
    "    }\n",
    "    print(\"Loaded Sentence Transformer for TextRank.\")\n",
    "\n",
    "    print(\"\\nAll models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}. Please check your token and model access permissions.\")\n",
    "    # Removed exit() so the cell doesn't stop execution completely\n",
    "    # exit()\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79fcf513",
    "outputId": "5abca152-9f7f-4505-f2bd-ad6c8d211e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate and rouge_score installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score --quiet\n",
    "print(\"evaluate and rouge_score installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9shiZcICR19"
   },
   "source": [
    "Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEqiSOwGCTcu"
   },
   "source": [
    "TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlhS3NZTCVOC",
    "outputId": "b72629de-bd51-4c24-8bf0-fafcdcddb32e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama Summary:\n",
      " Infosys TextMorph Milestone 2 focuses on advanced text summarization using both abstractive and extractive approaches, evaluation using ROUGE metrics, interactive Google Colab UIs, and testing on 10+ domain-specific texts. The milestone also includes visualizing performance through comparative plots.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALL ---\n",
    "# !pip install transformers torch\n",
    "\n",
    "# --- IMPORTS ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- FUNCTION ---\n",
    "def summarize_tinyllama(text):\n",
    "    chat = [{\"role\": \"user\", \"content\": f\"Summarize this text concisely:\\n\\n{text}\"}]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "# --- SAMPLE TEXT ---\n",
    "text = \"\"\"\n",
    "Infosys TextMorph Milestone 2 focuses on advanced text summarization (both abstractive and extractive),\n",
    "evaluation using ROUGE metrics, interactive Google Colab UIs, testing on 10+ domain-specific texts,\n",
    "and visualizing performance through comparative plots.\n",
    "\"\"\"\n",
    "\n",
    "# --- RUN ---\n",
    "summary = summarize_tinyllama(text)\n",
    "print(\"TinyLlama Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xK7_yHXCZfe"
   },
   "source": [
    "Phi (microsoft/phi-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505,
     "referenced_widgets": [
      "512c4ad068714f94a163a0fba4babb06",
      "26416ece23a048578378102ddd57d9a5",
      "5c02f9c4d36d4b3591b628fe18cb9748",
      "d447aa40338a44c3adab3bdcd38c3a26",
      "c8e289f6800943eab93b360bb777f168",
      "761a9124abbc4e14908bf37c676af548",
      "865222c426eb4e449caf9dc2a74befe6",
      "90b3b30c81934e408e084b239e3cd1d5",
      "f35c62251be14807acd9ec6a363fa384",
      "b288fcdf24164d7b9ba38d9779ec0b41",
      "3ae74df385aa4b4bab7c11807fda10b2",
      "b809214e69504ccb854c9a6a1b50c539",
      "d3dfab6b098f4048aa28f4404c85b5e4",
      "a67ae78330324864beb3073bd36933ed",
      "22c3bec5d65e424394f266f1a2feb3bf",
      "e0333705f14040799268c405f0696b34",
      "f7f2bbb9250442c4b82aa215bc2246ba",
      "57a67f69ae604cb0a02153a403b6f491",
      "a7d197b2b2cd4b1b9cb84497ef0a9835",
      "5c7948faf5ed4ddb8eaea9d966b9cf0f",
      "79c5ed56dd31414794ff41b503c4816f",
      "908d37ccafcd4cd5a322e1d3746e7de5",
      "2fdcf10945094187a8990766f96126a4",
      "9b5c4881122743fa9a76f8c51a04dbcf",
      "7d3b4550758d40908890641b54060bd9",
      "cb8e55487ac44014889e23d24a245348",
      "15aeb1fa7e98407e8f9681a0309e3e12",
      "b12bdf21468a48c1b0cf0601dcf4e0bb",
      "7d7cc96d81624aea836ece9d4b7a003b",
      "fb28f284ca864fe0b77822a17543d449",
      "2f273ced4dc34b5992da46bcc5f64a27",
      "64e4834d40d34879a34050dadbb00269",
      "e03613c2a9ef4e6ebd31870efddf314d",
      "be101f6c0edc4cc9b1e9a5f249f0ef62",
      "47341ecd541441ecaadf07542b18dc8e",
      "b229216f4c6d41769a51d63fcc0540e0",
      "4561931eeb5647b8b0a31deda31f11ce",
      "c3d68cea4b964a40a63edbc2c7984b94",
      "f5c8c8a9eb1449b0a38b168acb7e39d9",
      "c56ae5ccc52e4133aaf2cac84c58f8ae",
      "cad518cd39d74ab19e26ee9ed7109757",
      "f4fa7b4e1891464e9091ecf497593d0d",
      "c5f0306e7215478994e08313ba390dd4",
      "7c3de6e7aee846218cf4225d328f29df",
      "d4fb4bb314354f31a3b855d97eb892b4",
      "e8acf7fc775640749c29d47d575ef610",
      "199c9f77cd4e4dfab992a59b84c30540",
      "e23ae8bd958943a1953645b322473f3a",
      "8f0d0f86891842f4ae050bbae66bc7b9",
      "f6a9deb8e4984aeeb97e1bc11cb8f90f",
      "ea96b4ee4fb74794b285c6461c292f3f",
      "32a65f5a95d5443e809fba594417b370",
      "619b5c15898d4731bfb8c6d5d3a85850",
      "8fc46c1f20524f918551ddf32a93da27",
      "4b4632ce6e814e00b4255e0869f35845",
      "3071fa53780c484ca7ec4102ed9dbbd5",
      "5482f34e9db84f6686026d44f8b146f8",
      "35d9024cd53d47b3bf5552cfd18532bd",
      "18f120d3fd1b4a7db0b442b084d0d52b",
      "e8d7cd5174a54b2485d3d9de77907e21",
      "e968c2f7786d4c17b286edb544e1d9dc",
      "e3925a091bbb48ffbb64f0a26a3793e7",
      "465c09b8984f407092e366c1c5fbc3af",
      "96019b6862374d6c8a82f4da3f6a7932",
      "5235f599d4c348cdafb0a7a673ad784b",
      "d62e75e13dcf427c93a91741e4c3bcaa",
      "115de63abaa7435d858d4888964cc8fa",
      "878e87ecffa44f1d9ad616b6b4b09a1a",
      "7bf0c816c9c843f0b374202542723d00",
      "8010d00751624319996b73f6cba3aec4",
      "c0b0393b864e4140962b88c15196070a",
      "c1f8d51b4fa047b3af656175c223a9d9",
      "3c7b3f985abd4516b877f7a51c8f6ed1",
      "454089167a924cd7a2d746dc9a6760d8",
      "6e931e42700549d2acdcef41a1d4dca9",
      "b1b7d7dcb8484309a2929db85ef2a46b",
      "a4d5167b16cf4bf09b6c84624cba4439",
      "a4c9fed6808c44439ace9c6f766111c8",
      "16ce263610fa4a9a8489714fbe138a86",
      "30908ac93b834b0194ae7c74991f433d",
      "8783258a3bef4d4d8282a3ba4c9bd0ff",
      "a03d14f681ec45ccb9996d064dee00bd",
      "d964a664648840ba8267a76b1a084168",
      "a0cb11e432844550b20cb8f4dae2ee50",
      "722b358565554fd8994d94db39c5da58",
      "c6201dff61b4479d87cab4666af360be",
      "557750994b2242319de70d9f6ce32666",
      "a1ff560109e04463b5307f402a1c7269",
      "06a12f23172242ea8c3f8f018039fcaf",
      "a45a404baf484e58a49dd8908dcb67b5",
      "d7b483a9b8744c34b5327445259c6f0c",
      "a94d3d87ff4348edbd7893fd40635444",
      "614edbcee38941f9a721fb3f287ab07b",
      "2555b42a4e5146e18998dd1c756fca0a",
      "0c31acdfc67942c4b1fbb43f299d70dc",
      "0c56dd3245804ac7a6df040423d75398",
      "4cd2fbf91cda437997654a13848c50b0",
      "b6266bad175744f7beb4d0c26c4a796a",
      "0876db98948c453bb229ca61bd42fd47",
      "216388f89af0489c9df067fc962553ce",
      "9fc15051a1494de38c08a1517e5c157b",
      "1e6338bbc60f4d7c82a2c39af6cd224a",
      "5ea51652d7034004bd87ce7c53fdc2fd",
      "6873d19a89354385940c0ef8c19f85f5",
      "512e86eb712b41dc9fa3f9b66cc8c4dc",
      "7b432ab1ec6f4a4fb7ee2931adfbda27",
      "3b74e3d3c28d4ef0a26e74aac063daae",
      "11f7b55e782d4c5697a6a1a769a82314",
      "7efd7504ff2f47ecb40c9e4f29dd7509",
      "dea4eee481e540cca804d7cc1635226c",
      "0586dec92c3049bab77d6e94dd4327c5",
      "971a1bf5f04d4ce9aa1aea86fc9678e2",
      "3ff2010c9f36406a80fcb9dbb080c7bf",
      "ad1a127182ed40c5b16bb5b839d9a615",
      "c3595b3bb8ae422c944a35566fee9f43",
      "6593bdb78d614f7eac6de2e98d2cc383",
      "98cb18dadcee424ab644f18bb1e9e8a3",
      "827952f9ecc446a1a928d85934b321ac",
      "f32757ffcb7a4a82836e5ffff9af2986",
      "e3fc233df1cb46d7b0a5a12dfe82dae6",
      "c65081d94fb44eee98f1177fc1601a44",
      "53e1253841d749d6ba1f9a713719ea53",
      "e3214712fcc4404cbe5067620b1757a3",
      "2378c0c87bf348f18eefb347c1dfa3e6",
      "89b1fb28ff7149dd91455fdb2a715634",
      "ef5ff6977c5d4b43afed0dbf3ac235dd",
      "7412654a74ce4f83806c250904125eba",
      "cbff4ff914a4449bb9f89bf20f8e9203",
      "094744b1535840ee829262b621ad258a",
      "8c23a655e022486d9ccd9c7d06e30b3b",
      "ecb6fbd965d7426da52e329a349d5c26",
      "86ac5d92bc8b4dd6a95e41ad1526d2fa",
      "f778cd035465461c93e68c281d551d2c",
      "1052f581440640c28f8acd9b1a7bbd89",
      "3a325e1d0c584b6891f972b08ad35798",
      "8c4e08c7586842f0ad17852b6c79f88c",
      "914d14b8764c4d88969d78a2bc1d9c20",
      "d9d19c141c154f73a38e0ea4ba0d7e07",
      "36a252b6de00489091c7ff28f113d84c",
      "b92d3011888f4900bbe90ad57eca969c",
      "417a6d4dca8c4c339a249e8d598d2a62",
      "b3cfcdaa2d6141b2bf897514bdbf45ea",
      "c1e36cc1ebdf486cae29ec23b700731d"
     ]
    },
    "id": "Hm2lqN-YCSRQ",
    "outputId": "8294c7cf-a3e4-40dc-b25a-47370cbf118f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512c4ad068714f94a163a0fba4babb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809214e69504ccb854c9a6a1b50c539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdcf10945094187a8990766f96126a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be101f6c0edc4cc9b1e9a5f249f0ef62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fb4bb314354f31a3b855d97eb892b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3071fa53780c484ca7ec4102ed9dbbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115de63abaa7435d858d4888964cc8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c9fed6808c44439ace9c6f766111c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a12f23172242ea8c3f8f018039fcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216388f89af0489c9df067fc962553ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0586dec92c3049bab77d6e94dd4327c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e1253841d749d6ba1f9a713719ea53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f778cd035465461c93e68c281d551d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Phi Summary:\n",
      " The Infosys TextMorph program milestone highlights the development of abstractive and extractive summarization models, metric-based evaluation, UI creation in Google Colab, testing on different domains, and visualization of outcomes.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALL ---\n",
    "# !pip install transformers torch\n",
    "\n",
    "# --- IMPORTS ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- FUNCTION ---\n",
    "def summarize_phi(text):\n",
    "    prompt = f\"Instruct: Summarize the following text concisely.\\n{text}\\nOutput:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Output:\" in decoded:\n",
    "        return decoded.split(\"Output:\")[1].strip()\n",
    "    return decoded\n",
    "\n",
    "# --- SAMPLE TEXT ---\n",
    "text = \"\"\"\n",
    "The Infosys TextMorph program milestone emphasizes abstractive and extractive summarization models,\n",
    "metric-based evaluation, UI creation in Google Colab, testing on various domains, and visualization of outcomes.\n",
    "\"\"\"\n",
    "\n",
    "# --- RUN ---\n",
    "summary = summarize_phi(text)\n",
    "print(\" Phi Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW2xUvxrCgaW"
   },
   "source": [
    "BART-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va6Ab3jMCizq",
    "outputId": "58264d7b-ad32-4897-807a-18240e002d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Summary:\n",
      " Milestone 2 of Infosys TextMorph aims to advance summarization through modern transformer models,evaluate them with standardized metrics, and provide a user-friendly interactive platform to visualize results across multiple domains.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# --- FUNCTION ---\n",
    "def summarize_bart(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=120,\n",
    "        min_length=40,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# --- SAMPLE TEXT ---\n",
    "text = \"\"\"\n",
    "Milestone 2 of Infosys TextMorph aims to advance summarization through modern transformer models,\n",
    "evaluate them with standardized metrics, and provide a user-friendly interactive platform\n",
    "to visualize results across multiple domains.\n",
    "\"\"\"\n",
    "\n",
    "# --- RUN ---\n",
    "summary = summarize_bart(text)\n",
    "print(\"BART Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONPr3DtuCkqu"
   },
   "source": [
    "Gemma (google/gemma-2b-it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "481cd4eb880a4d169a7676242e160bbf",
      "098328bdde9a4822b100ecef73fb1aa6",
      "877b487ad9444265ae45c2a3ef9d427f",
      "a486cf5af84544bdab6fd18969ec7162",
      "6e1959710c5d492892c1225fb82bf33d",
      "c7e3ea59a80244ab8f75f2adda917f7a",
      "1644950d784c4a26a2ecb6844252a969",
      "7efcf88e76244019afe0b5ff994d8f5a",
      "50f20296434a4a4ab74f8223905b2f7a",
      "e22fcfe58b4d49d588b2ebd4cb9cbbf5",
      "f7e76980c5234699809c5767eef2c8c8",
      "c530803c8b784aa8b783230a645473ca",
      "a83a9a4464274a1d8a2ab0df3f4d6899",
      "da935b0ba1504f6f8038d2a3fd166a69",
      "40e63f8243cd4bd089db0d02e1c33823",
      "964944d359394507aec995d0e986d9b4",
      "e36e5917a31e42e6a4d2f86393ee31f8",
      "c088a8245be3437a82c5769c115f8af9",
      "e12690c534134c378305c32e163f070f",
      "04dd2a7a713b43d48480b8ec5ad04473",
      "09488c6d871441bb947d323318e592f5",
      "116a1326560a4172b4d8169f9e860a33"
     ]
    },
    "id": "8Yv0kErkBUGE",
    "outputId": "a6409324-7022-43f6-a072-f9c111fb8586"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481cd4eb880a4d169a7676242e160bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530803c8b784aa8b783230a645473ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma Summary:\n",
      " Summarize the following text concisely and clearly:\n",
      "\n",
      "\n",
      "Infosys TextMorph Milestone 2 focuses on implementing abstractive summarization models like TinyLlama, Phi, BART, and Gemma,\n",
      "evaluating their performance through metrics and readability, and integrating them into an interactive Colab UI for visualization.\n",
      "The project aims to explore the potential of these models for text generation, summarization, and information extraction.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# --- FUNCTION ---\n",
    "def summarize_gemma(text):\n",
    "    prompt = f\"Summarize the following text concisely and clearly:\\n\\n{text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# --- SAMPLE TEXT ---\n",
    "text = \"\"\"\n",
    "Infosys TextMorph Milestone 2 focuses on implementing abstractive summarization models like TinyLlama, Phi, BART, and Gemma,\n",
    "evaluating their performance through metrics and readability, and integrating them into an interactive Colab UI for visualization.\n",
    "\"\"\"\n",
    "\n",
    "# --- RUN ---\n",
    "summary = summarize_gemma(text)\n",
    "print(\"Gemma Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWVft4B-EMlX"
   },
   "source": [
    "TextRank – Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9YD-ohoYM_1",
    "outputId": "1fc3a90d-a74c-4985-d667-964e92c4f278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank Extractive Summary:\n",
      "\n",
      "The Infosys TextMorph Milestone 2 focuses on advanced text summarization methods,\n",
      "The milestone also requires building\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALL ---\n",
    "!pip install summa --quiet\n",
    "\n",
    "from summa import summarizer\n",
    "\n",
    "# --- FUNCTION ---\n",
    "def summarize_textrank(text, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Perform extractive summarization using TextRank from the summa library.\n",
    "    Extracts approximately 'num_sentences' most important sentences from the text.\n",
    "    \"\"\"\n",
    "    # Get list of important sentences\n",
    "    summarized_sentences = summarizer.summarize(text, ratio=0.3, split=True)\n",
    "\n",
    "    # Select the top N sentences\n",
    "    top_sentences = summarized_sentences[:num_sentences]\n",
    "\n",
    "    # Join them into one string\n",
    "    return \"\\n\".join(top_sentences)\n",
    "\n",
    "\n",
    "# --- SAMPLE TEXT ---\n",
    "text = \"\"\"\n",
    "The Infosys TextMorph Milestone 2 focuses on advanced text summarization methods,\n",
    "including both abstractive and extractive techniques. Interns are expected to evaluate models\n",
    "using metrics like ROUGE and semantic similarity. The milestone also requires building\n",
    "interactive Google Colab UIs, testing on more than 10 sample texts from various domains,\n",
    "and visualizing the model performance through comparative plots such as bar and radar charts.\n",
    "\"\"\"\n",
    "\n",
    "# --- RUN ---\n",
    "summary = summarize_textrank(text, num_sentences=3)\n",
    "print(\"TextRank Extractive Summary:\\n\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4iZOH7iEjF3"
   },
   "source": [
    "Evaluation Metrics Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dBG4g0uFW3f",
    "outputId": "78e49c19-3991-4861-da93-a528469169be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.10-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
      "Downloading textstat-0.7.10-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.17.2 textstat-0.7.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ROUGE Scores:\n",
      "ROUGE1 -> Precision: 0.6875, Recall: 0.5500, F1: 0.6111\n",
      "ROUGE2 -> Precision: 0.2667, Recall: 0.2105, F1: 0.2353\n",
      "ROUGEL -> Precision: 0.5625, Recall: 0.4500, F1: 0.5000\n",
      "\n",
      " Semantic Similarity: 0.8526\n",
      "\n",
      " Readability Metrics:\n",
      "Flesch Reading Ease: 61.89\n",
      "Flesch-Kincaid Grade Level: 8.35\n",
      "SMOG Index: 13.02\n",
      "Automated Readability Index: 9.93\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install rouge-score sentence-transformers textstat nltk\n",
    "\n",
    "#  Import libraries\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import textstat\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example ground truth and generated summaries\n",
    "reference_summary = \"\"\"Artificial Intelligence is a branch of computer science that enables machines to mimic human intelligence, including learning and decision-making.\"\"\"\n",
    "generated_summary = \"\"\"AI is a field in computer science that helps machines learn and make human-like decisions.\"\"\"\n",
    "\n",
    "# -------------------------------\n",
    "# 🔹 1. ROUGE SCORE EVALUATION\n",
    "# -------------------------------\n",
    "def calculate_rouge(reference, generated):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    print(\"\\n ROUGE Scores:\")\n",
    "    for metric, result in scores.items():\n",
    "        print(f\"{metric.upper()} -> Precision: {result.precision:.4f}, Recall: {result.recall:.4f}, F1: {result.fmeasure:.4f}\")\n",
    "    return scores\n",
    "\n",
    "rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "\n",
    "# -------------------------------\n",
    "# 🔹 2. SEMANTIC SIMILARITY\n",
    "# -------------------------------\n",
    "def calculate_semantic_similarity(reference, generated):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings1 = model.encode(reference, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(generated, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(embeddings1, embeddings2)\n",
    "    print(f\"\\n Semantic Similarity: {similarity.item():.4f}\")\n",
    "    return similarity.item()\n",
    "\n",
    "semantic_score = calculate_semantic_similarity(reference_summary, generated_summary)\n",
    "\n",
    "# -------------------------------\n",
    "# 🔹 3. READABILITY METRICS\n",
    "# -------------------------------\n",
    "def calculate_readability(text):\n",
    "    print(\"\\n Readability Metrics:\")\n",
    "    print(f\"Flesch Reading Ease: {textstat.flesch_reading_ease(text):.2f}\")\n",
    "    print(f\"Flesch-Kincaid Grade Level: {textstat.flesch_kincaid_grade(text):.2f}\")\n",
    "    print(f\"SMOG Index: {textstat.smog_index(text):.2f}\")\n",
    "    print(f\"Automated Readability Index: {textstat.automated_readability_index(text):.2f}\")\n",
    "\n",
    "calculate_readability(generated_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27Y3INJtFd0e"
   },
   "source": [
    "UI AND UX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJalkEERFgwx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64rgU1-4GGHx",
    "outputId": "0241ebc8-64a8-4f4b-a0bd-9ba39def1668"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully and ready for UI!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup\n",
    "!pip install gradio transformers sentencepiece --quiet\n",
    "\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# Load model (you can change to TinyLlama, Pegasus, or T5)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(\"✅ Model loaded successfully and ready for UI!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HctOi-xcGKy3"
   },
   "source": [
    "Build Interactive UI Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "cuO4qzhbGLXr",
    "outputId": "f278d50c-232a-4099-f1aa-41a146b221c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://9a5116188731af38af.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9a5116188731af38af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Define summarization function\n",
    "def summarize_text(text, model_choice):\n",
    "    if not text.strip():\n",
    "        return \" Please enter some text to summarize!\"\n",
    "\n",
    "    # (Optional) Switch models dynamically\n",
    "    if model_choice == \"BART\":\n",
    "        summarizer.model = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\").model\n",
    "    elif model_choice == \"T5\":\n",
    "        summarizer.model = pipeline(\"summarization\", model=\"t5-base\").model\n",
    "    elif model_choice == \"Pegasus\":\n",
    "        summarizer.model = pipeline(\"summarization\", model=\"google/pegasus-xsum\").model\n",
    "    elif model_choice == \"TinyLlama\":\n",
    "        summarizer.model = pipeline(\"summarization\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\").model\n",
    "\n",
    "    summary = summarizer(text, max_length=120, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Create dropdown for model selection\n",
    "model_selector = gr.Dropdown(\n",
    "    [\"BART\", \"T5\", \"Pegasus\", \"TinyLlama\"],\n",
    "    value=\"BART\",\n",
    "    label=\"Choose Summarization Model\"\n",
    ")\n",
    "\n",
    "# Create textbox for input and output\n",
    "input_box = gr.Textbox(\n",
    "    label=\"Enter or Paste Text\",\n",
    "    placeholder=\"Paste your paragraph, article, or report here...\",\n",
    "    lines=10\n",
    ")\n",
    "\n",
    "output_box = gr.Textbox(\n",
    "    label=\"Generated Summary\",\n",
    "    placeholder=\"Summary will appear here...\",\n",
    "    lines=8\n",
    ")\n",
    "\n",
    "# Build Gradio interface\n",
    "ui = gr.Interface(\n",
    "    fn=summarize_text,\n",
    "    inputs=[input_box, model_selector],\n",
    "    outputs=output_box,\n",
    "    title=\" Infosys TextMorph - Advanced Text Summarizer\",\n",
    "    description=\"Experiment with abstractive and extractive summarization models like BART, T5, Pegasus, and TinyLlama.\",\n",
    "    theme=\"soft\",\n",
    "    examples=[\n",
    "        [\"The Indian economy is expected to grow at 7.2% this year according to RBI reports.\", \"BART\"],\n",
    "        [\"Lionel Messi scored two goals leading Argentina to victory in the Copa America final.\", \"Pegasus\"],\n",
    "        [\"Apple announced its new AI chip that enhances device performance by 60%.\", \"T5\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "ui.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efofalKdGMy7"
   },
   "source": [
    "Add Evaluation & Style Enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "PyFAYCXLGQKj",
    "outputId": "3cc96493-2202-4747-a949-1fdff0f6938e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://345dc099adb9d91182.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://345dc099adb9d91182.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Add readability and length metrics (optional)\n",
    "import textstat\n",
    "\n",
    "def summarize_with_eval(text, model_choice):\n",
    "    summary = summarize_text(text, model_choice)\n",
    "    readability = textstat.flesch_reading_ease(summary)\n",
    "    compression = round((1 - len(summary) / len(text)) * 100, 2)\n",
    "\n",
    "    result = f\"**Summary:**\\n{summary}\\n\\n\"\n",
    "    result += f\" **Readability Score:** {readability}\\n\"\n",
    "    result += f\" **Compression:** {compression}%\"\n",
    "    return result\n",
    "\n",
    "ui_eval = gr.Interface(\n",
    "    fn=summarize_with_eval,\n",
    "    inputs=[input_box, model_selector],\n",
    "    outputs=\"text\",\n",
    "    title=\" Infosys TextMorph - Summarization with Evaluation\",\n",
    "    description=\"View readability and compression metrics along with your summary.\",\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "# Launch enhanced interface\n",
    "ui_eval.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2XlbDWmGTGw"
   },
   "source": [
    "Streamlit UI/UX Interaction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odsszqITGIet",
    "outputId": "b1e3324d-f6ed-41aa-8123-456bdee2cdb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/10.1 MB\u001b[0m \u001b[31m176.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m192.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m267.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install streamlit transformers sentencepiece textstat rouge-score --quiet\n",
    "# Import libraries\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA4CTQd-Gb0C",
    "outputId": "51d42310-f5d5-4c6a-94af-619d09ae07e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 15:43:29.432 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.434 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.902 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
      "2025-10-14 15:43:29.905 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.916 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.918 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.921 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.926 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.927 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.939 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-14 15:43:29.942 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.949 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.950 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.954 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.960 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.962 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.965 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.969 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.973 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-14 15:43:29.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Page configuration\n",
    "st.set_page_config(page_title=\"Infosys TextMorph Summarizer\", layout=\"wide\")\n",
    "\n",
    "# Title & description\n",
    "st.title(\" Infosys TextMorph - Advanced Text Summarizer\")\n",
    "st.write(\"Enter your text, choose a summarization model, and generate summaries with metrics.\")\n",
    "\n",
    "# Sidebar for model selection\n",
    "model_choice = st.sidebar.selectbox(\n",
    "    \"Select Summarization Model\",\n",
    "    [\"BART\", \"T5\", \"Pegasus\", \"TinyLlama\"],\n",
    "    index=0\n",
    ")\n",
    "\n",
    "# Input text area\n",
    "input_text = st.text_area(\n",
    "    \"Paste or type your text here:\",\n",
    "    height=200,\n",
    "    placeholder=\"Enter any article, report, or paragraph...\"\n",
    ")\n",
    "\n",
    "# Summarize button\n",
    "if st.button(\"Generate Summary\"):\n",
    "    if not input_text.strip():\n",
    "        st.warning(\"⚠️ Please enter some text to summarize!\")\n",
    "    else:\n",
    "        # Load model based on selection\n",
    "        if model_choice == \"BART\":\n",
    "            summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        elif model_choice == \"T5\":\n",
    "            summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "        elif model_choice == \"Pegasus\":\n",
    "            summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
    "        elif model_choice == \"TinyLlama\":\n",
    "            summarizer = pipeline(\"summarization\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "        # Generate summary\n",
    "        summary = summarizer(input_text, max_length=120, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "        # Display results\n",
    "        st.subheader(\" Summary\")\n",
    "        st.write(summary)\n",
    "\n",
    "        # Optional: Display evaluation metrics\n",
    "        readability = textstat.flesch_reading_ease(summary)\n",
    "        compression = round((1 - len(summary)/len(input_text))*100, 2)\n",
    "        st.subheader(\" Summary Metrics\")\n",
    "        st.write(f\"**Readability Score:** {readability}\")\n",
    "        st.write(f\"**Compression Ratio:** {compression}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "256953fbbfc047eb963f61bce7b7ed0b",
      "ffea1d4e66c845498ba08cec258b2a18",
      "463df131fe9141349a20f6998e01a03a",
      "d5b22605cae24f13b71b8bd689d0c06f",
      "0ec9c1bbfe8a4b8da2aad0700943c575",
      "278c410ca0324283816feaed2139fddc",
      "4b4601f325c04d78969113a6099eaad8",
      "4132266a7488492f83b2d5d3ec6d5d6c",
      "a473fc5b2d6c4af5a6bae00243822d44",
      "4a5d6d31345444fdb787de4650486d53"
     ]
    },
    "id": "SkX2e-Q6GjwN",
    "outputId": "1d7f6066-c0cb-42d1-9b5a-56f035eaedf6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256953fbbfc047eb963f61bce7b7ed0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Input:', layout=Layout(height='150px', width='100%'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------ UI 1 ------------------\n",
    "import ipywidgets as widgets # Import ipywidgets\n",
    "from IPython.display import display, clear_output # Import display and clear_output\n",
    "\n",
    "stext_input_all = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Paste your text here...\",\n",
    "    description=\"Input:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"150px\")\n",
    ")\n",
    "\n",
    "btn_summarize_all = widgets.Button(description=\"Summarize with All Models\", button_style=\"success\")\n",
    "output_all = widgets.Output()\n",
    "\n",
    "def summarize_all_models(b):\n",
    "    with output_all:\n",
    "        clear_output()\n",
    "        text = stext_input_all.value.strip()\n",
    "        if not text:\n",
    "            print(\" Please enter some text!\")\n",
    "            return\n",
    "\n",
    "        print(\" Generating summaries...\\n\")\n",
    "\n",
    "        models = {\n",
    "            \"BART\": pipeline(\"summarization\", model=\"facebook/bart-large-cnn\"),\n",
    "            \"TinyLlama\": pipeline(\"summarization\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "            \"Phi\": pipeline(\"summarization\", model=\"bigscience/phi-1.0\"),\n",
    "            \"Gemma\": pipeline(\"summarization\", model=\"google/gemma-2b-it\"),\n",
    "            \"TextRank\": \"textrank\"\n",
    "        }\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\" Model: {name}\")\n",
    "            if name == \"TextRank\":\n",
    "                # Assuming textrank_summarizer is defined in a previous cell\n",
    "                summary = textrank_summarizer.summarize(text)\n",
    "            else:\n",
    "                summary = model(text, max_length=120, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "            readability = textstat.flesch_reading_ease(summary)\n",
    "            compression = round((1 - len(summary)/len(text))*100, 2)\n",
    "\n",
    "            print(f\"Summary:\\n{summary}\")\n",
    "            print(f\"Readability Score: {readability}\")\n",
    "            print(f\"Compression: {compression}%\\n\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "btn_summarize_all.on_click(summarize_all_models)\n",
    "\n",
    "display(widgets.VBox([stext_input_all, btn_summarize_all, output_all]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC-PnRAFGoGt"
   },
   "source": [
    "Summarize with Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375,
     "referenced_widgets": [
      "712407d493a24893be319653805be1ce",
      "e4610a655dd741a08c3777d1c708ac87",
      "cdd8ac4881854301a058c32cb190e21b",
      "86eae840262848f1af2f5fd06e5c6797",
      "22ba00528e974ae6a3478e2871a4d5dc",
      "02d409145b344c33816b814aa19ae86f",
      "e400f033ea2b48f1a62a7275c0a50899",
      "52ca335adbd244a1be493ba23a247f8f",
      "0970e267177245e3a55869f45c24b37f",
      "59b9a47cdc8244e6bb99beddcc273569",
      "a16d2a53c1e64335a7d86897e36c58da",
      "5aee94c39a4749ed97ded1665d77151f",
      "516db4e6b953448cb31f46ea27d46846",
      "96d312fbbce74aaabb7fa673a1880bbd",
      "766935296652400bb0dc064784645444",
      "5559a2a364fe497c80b9d0aa0a3a52c4",
      "984e5518028f4540ab5f41c79360b87d",
      "63e5cfc1e7cd4918821f914ae54a071e",
      "007683612f5d4f619c6739f851bd5414",
      "fb47e83db7134bb5ac95e5721c9f8f97",
      "d2b3145aaa7b4fd68194d8cf69d900b2",
      "5a47b16ebb28412983b0af9cd8d22ee7",
      "9ea1ff3d564d49c7ac1a2d4f6fa0ff58",
      "5744b7cc72b8482384f15e15eed62883",
      "c5b24748d92642d5a75741829f7b570c",
      "0802c1831335483fb1c073cc41fcedae",
      "176c8cce83c04443a0bf2252ba5a7ebd"
     ]
    },
    "id": "H1Bfh0TrGkv7",
    "outputId": "b16d4acb-029f-48e2-f46a-5c181a0e6a96"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712407d493a24893be319653805be1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Input:', layout=Layout(height='150px', width='100%'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------------------ UI 2 ------------------\n",
    "stext_input_sel = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Paste your text here...\",\n",
    "    description=\"Input:\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"150px\")\n",
    ")\n",
    "\n",
    "# Checkbox selection for models\n",
    "model_checkboxes = widgets.VBox([\n",
    "    widgets.Checkbox(value=True, description='BART'),\n",
    "    widgets.Checkbox(value=True, description='TinyLlama'),\n",
    "    widgets.Checkbox(value=True, description='Phi'),\n",
    "    widgets.Checkbox(value=True, description='Gemma'),\n",
    "    widgets.Checkbox(value=True, description='TextRank')\n",
    "])\n",
    "\n",
    "btn_summarize_sel = widgets.Button(description=\"Summarize Selected Models\", button_style=\"info\")\n",
    "output_sel = widgets.Output()\n",
    "\n",
    "def summarize_selected_models(b):\n",
    "    with output_sel:\n",
    "        clear_output()\n",
    "        text = stext_input_sel.value.strip()\n",
    "        if not text:\n",
    "            print(\"Please enter some text!\")\n",
    "            return\n",
    "\n",
    "        selected_models = [cb.description for cb in model_checkboxes.children if cb.value]\n",
    "        if not selected_models:\n",
    "            print(\"Please select at least one model!\")\n",
    "            return\n",
    "\n",
    "        print(\" Generating summaries...\\n\")\n",
    "\n",
    "        model_pipelines = {\n",
    "            \"BART\": pipeline(\"summarization\", model=\"facebook/bart-large-cnn\"),\n",
    "            \"TinyLlama\": pipeline(\"summarization\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "            \"Phi\": pipeline(\"summarization\", model=\"bigscience/phi-1.0\"),\n",
    "            \"Gemma\": pipeline(\"summarization\", model=\"google/gemma-2b-it\"),\n",
    "            \"TextRank\": \"textrank\"\n",
    "        }\n",
    "\n",
    "        for name in selected_models:\n",
    "            print(f\"📌 Model: {name}\")\n",
    "            model = model_pipelines[name]\n",
    "            if name == \"TextRank\":\n",
    "                summary = textrank_summarizer.summarize(text)\n",
    "            else:\n",
    "                summary = model(text, max_length=120, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "            readability = textstat.flesch_reading_ease(summary)\n",
    "            compression = round((1 - len(summary)/len(text))*100, 2)\n",
    "\n",
    "            print(f\"Summary:\\n{summary}\")\n",
    "            print(f\"Readability Score: {readability}\")\n",
    "            print(f\"Compression: {compression}%\\n\")\n",
    "            print(\"-\"*60)\n",
    "\n",
    "btn_summarize_sel.on_click(summarize_selected_models)\n",
    "\n",
    "display(widgets.VBox([stext_input_sel, model_checkboxes, btn_summarize_sel, output_sel]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6rHB-E5G4-w"
   },
   "source": [
    "Test using 10 sample texts from different domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf9AC7IqIWhr"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece textstat rouge-score summa --quiet\n",
    "\n",
    "from transformers import pipeline\n",
    "import textstat\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from summa import summarizer as textrank_summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SopJX9mGflNb",
    "outputId": "0a9555bb-de62-4db4-8190-4766c9cc65fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 54. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n",
      "Your max_length is set to 100, but your input_length is only 37. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "Your max_length is set to 100, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on sample texts...\n",
      "--- Processing Finance ---\n",
      "  Summarizing with TinyLlama...\n",
      "    Error: CUDA Out of Memory for model 'tinyllama'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with Phi-2...\n",
      "    Error: CUDA Out of Memory for model 'phi'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 71.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with BART...\n",
      "    Error: CUDA Out of Memory for model 'bart'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 71.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with TextRank...\n",
      "    Error: TextRank produced an empty summary.\n",
      "------------------------------\n",
      "--- Processing Sports ---\n",
      "  Summarizing with TinyLlama...\n",
      "    Error: CUDA Out of Memory for model 'tinyllama'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 71.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with Phi-2...\n",
      "    Error: CUDA Out of Memory for model 'phi'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with BART...\n",
      "    Error: CUDA Out of Memory for model 'bart'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with TextRank...\n",
      "    Error: TextRank produced an empty summary.\n",
      "------------------------------\n",
      "--- Processing Technology ---\n",
      "  Summarizing with TinyLlama...\n",
      "    Error: CUDA Out of Memory for model 'tinyllama'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with Phi-2...\n",
      "    Error: CUDA Out of Memory for model 'phi'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with BART...\n",
      "    Error: CUDA Out of Memory for model 'bart'. Try a smaller model or a runtime with more GPU memory. Details: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 5810 has 14.74 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 72.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  Summarizing with TextRank...\n",
      "    Error: TextRank produced an empty summary.\n",
      "------------------------------\n",
      "\n",
      "--- Evaluation Results ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "repr_error": "Out of range float values are not JSON compliant: nan",
       "type": "dataframe",
       "variable_name": "evaluation_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9ecb4db8-7412-4dec-a39d-29d1996b4200\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>File</th>\n",
       "      <th>Processing Time (s)</th>\n",
       "      <th>Error</th>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <th>Semantic Similarity</th>\n",
       "      <th>Readability (Flesch)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyLlama</td>\n",
       "      <td>Finance</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'tinyllama...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi-2</td>\n",
       "      <td>Finance</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'phi'. Try...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>Finance</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'bart'. Tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>Finance</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Error: TextRank produced an empty summary.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TinyLlama</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'tinyllama...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phi-2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'phi'. Try...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BART</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'bart'. Tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: TextRank produced an empty summary.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TinyLlama</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'tinyllama...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Phi-2</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'phi'. Try...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BART</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: CUDA Out of Memory for model 'bart'. Tr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Error: TextRank produced an empty summary.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ecb4db8-7412-4dec-a39d-29d1996b4200')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9ecb4db8-7412-4dec-a39d-29d1996b4200 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9ecb4db8-7412-4dec-a39d-29d1996b4200');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-86bcea7d-3a2b-4c59-b5bc-957400dd2669\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86bcea7d-3a2b-4c59-b5bc-957400dd2669')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-86bcea7d-3a2b-4c59-b5bc-957400dd2669 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_1a22e3b2-3ee3-44e2-8e9c-ab3c78dca6a0\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('evaluation_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_1a22e3b2-3ee3-44e2-8e9c-ab3c78dca6a0 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('evaluation_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        Model        File  Processing Time (s)  \\\n",
       "0   TinyLlama     Finance                 0.00   \n",
       "1       Phi-2     Finance                 0.00   \n",
       "2        BART     Finance                 0.00   \n",
       "3    TextRank     Finance                 0.01   \n",
       "4   TinyLlama      Sports                 0.00   \n",
       "5       Phi-2      Sports                 0.00   \n",
       "6        BART      Sports                 0.00   \n",
       "7    TextRank      Sports                 0.00   \n",
       "8   TinyLlama  Technology                 0.00   \n",
       "9       Phi-2  Technology                 0.00   \n",
       "10       BART  Technology                 0.00   \n",
       "11   TextRank  Technology                 0.00   \n",
       "\n",
       "                                                Error ROUGE-1 F1 ROUGE-2 F1  \\\n",
       "0   Error: CUDA Out of Memory for model 'tinyllama...       None       None   \n",
       "1   Error: CUDA Out of Memory for model 'phi'. Try...       None       None   \n",
       "2   Error: CUDA Out of Memory for model 'bart'. Tr...       None       None   \n",
       "3          Error: TextRank produced an empty summary.       None       None   \n",
       "4   Error: CUDA Out of Memory for model 'tinyllama...       None       None   \n",
       "5   Error: CUDA Out of Memory for model 'phi'. Try...       None       None   \n",
       "6   Error: CUDA Out of Memory for model 'bart'. Tr...       None       None   \n",
       "7          Error: TextRank produced an empty summary.       None       None   \n",
       "8   Error: CUDA Out of Memory for model 'tinyllama...       None       None   \n",
       "9   Error: CUDA Out of Memory for model 'phi'. Try...       None       None   \n",
       "10  Error: CUDA Out of Memory for model 'bart'. Tr...       None       None   \n",
       "11         Error: TextRank produced an empty summary.       None       None   \n",
       "\n",
       "   ROUGE-L F1 Semantic Similarity Readability (Flesch)  \n",
       "0        None                None                 None  \n",
       "1        None                None                 None  \n",
       "2        None                None                 None  \n",
       "3        None                None                 None  \n",
       "4        None                None                 None  \n",
       "5        None                None                 None  \n",
       "6        None                None                 None  \n",
       "7        None                None                 None  \n",
       "8        None                None                 None  \n",
       "9        None                None                 None  \n",
       "10       None                None                 None  \n",
       "11       None                None                 None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Average Metrics per Model (Excluding Errors) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "repr_error": "Out of range float values are not JSON compliant: nan",
       "type": "dataframe",
       "variable_name": "average_metrics"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-57f3b15d-5581-4129-a456-7acbb258dfb9\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Processing Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57f3b15d-5581-4129-a456-7acbb258dfb9')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-57f3b15d-5581-4129-a456-7acbb258dfb9 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-57f3b15d-5581-4129-a456-7acbb258dfb9');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_0958430a-610d-4a5f-a1e3-d48227ec7bc3\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('average_metrics')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_0958430a-610d-4a5f-a1e3-d48227ec7bc3 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('average_metrics');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Processing Time (s)]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- INSTALL LIBRARIES ---\n",
    "!pip install transformers rouge-score textstat sentence-transformers summa -q\n",
    "\n",
    "# --- IMPORTS ---\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "import textstat\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt', quiet=True) # Download punkt for textstat and summa\n",
    "\n",
    "# --- SAMPLE TEXTS ---\n",
    "sample_texts = {\n",
    "    \"Finance\": \"The Indian stock market surged today as banking and IT stocks led the rally. The Sensex rose by 450 points, closing above 67,000, while Nifty crossed 20,000. Analysts attribute the rally to strong quarterly earnings and robust GDP growth forecasts.\",\n",
    "    \"Sports\": \"Virat Kohli scored a magnificent century leading India to victory against Australia in the final ODI match. The win secured India’s top position in the ICC rankings.\",\n",
    "    \"Technology\": \"Google unveiled its latest AI model that integrates multimodal learning, combining text, image, and audio processing for improved contextual understanding.\"\n",
    "}\n",
    "\n",
    "# Ensure MODELS dictionary is available from a previous cell\n",
    "if 'MODELS' not in globals():\n",
    "    print(\"Error: MODELS dictionary not found. Please run the 'Load AI Models' cell first.\")\n",
    "    # Add a mechanism to stop execution if MODELS is not available\n",
    "    # raise SystemExit(\"Required MODELS dictionary not found.\")\n",
    "\n",
    "# Access loaded models from the MODELS dictionary\n",
    "bart_pipeline = MODELS.get('bart', {}).get('summarizer')\n",
    "tinyllama_model_info = MODELS.get('tinyllama')\n",
    "phi_model_info = MODELS.get('phi')\n",
    "gemma_model_info = MODELS.get('gemma')\n",
    "\n",
    "# TextRank (Extractive) using summa\n",
    "from summa import summarizer as summa_textrank_summarizer\n",
    "\n",
    "def textrank_summarize_summa(text, ratio=0.3):\n",
    "    \"\"\"\n",
    "    Perform extractive summarization using TextRank from the summa library.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summary = summa_textrank_summarizer.summarize(text, ratio=ratio)\n",
    "        if not summary.strip(): # Check for empty summary\n",
    "             return \"Error: TextRank produced an empty summary.\"\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"TextRank summarization failed: {e}\")\n",
    "        return f\"Error: TextRank summarization failed: {e}\"\n",
    "\n",
    "# --- EVALUATION FUNCTIONS ---\n",
    "# Ensure sentence_model and rouge_scorer are initialized (assuming they are in a previous cell)\n",
    "if 'sentence_model' not in globals():\n",
    "    try:\n",
    "        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"SentenceTransformer loaded for evaluation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SentenceTransformer: {e}\")\n",
    "        sentence_model = None # Set to None if loading fails\n",
    "\n",
    "if 'scorer' not in globals():\n",
    "     try:\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        print(\"ROUGE scorer initialized for evaluation.\")\n",
    "     except Exception as e:\n",
    "        print(f\"Error initializing ROUGE scorer: {e}\")\n",
    "        scorer = None # Set to None if initialization fails\n",
    "\n",
    "def calculate_metrics(reference, summary):\n",
    "    \"\"\"Calculates ROUGE, Semantic Similarity, Readability, and Compression.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # ROUGE\n",
    "    if scorer and reference and summary and not summary.startswith(\"Error:\"):\n",
    "        try:\n",
    "            rouge_scores = scorer.score(reference, summary)\n",
    "            metrics[\"ROUGE-1 F1\"] = rouge_scores['rouge1'].fmeasure\n",
    "            metrics[\"ROUGE-2 F1\"] = rouge_scores['rouge2'].fmeasure\n",
    "            metrics[\"ROUGE-L F1\"] = rouge_scores['rougeL'].fmeasure\n",
    "        except Exception as e:\n",
    "            metrics[\"ROUGE-1 F1\"] = None\n",
    "            metrics[\"ROUGE-2 F1\"] = None\n",
    "            metrics[\"ROUGE-L F1\"] = None\n",
    "            print(f\"Error calculating ROUGE: {e}\")\n",
    "    else:\n",
    "         metrics[\"ROUGE-1 F1\"] = None\n",
    "         metrics[\"ROUGE-2 F1\"] = None\n",
    "         metrics[\"ROUGE-L F1\"] = None\n",
    "         if not reference or not summary or summary.startswith(\"Error:\"):\n",
    "             print(\"Reference or summary is empty/errored, cannot calculate ROUGE.\")\n",
    "\n",
    "\n",
    "    # Semantic similarity\n",
    "    if sentence_model and reference and summary and not summary.startswith(\"Error:\"):\n",
    "        try:\n",
    "            emb_ref = sentence_model.encode(reference, convert_to_tensor=True)\n",
    "            emb_sum = sentence_model.encode(summary, convert_to_tensor=True)\n",
    "            metrics[\"Semantic Similarity\"] = util.cos_sim(emb_ref, emb_sum).item()\n",
    "        except Exception as e:\n",
    "             metrics[\"Semantic Similarity\"] = None\n",
    "             print(f\"Error calculating Semantic Similarity: {e}\")\n",
    "    else:\n",
    "        metrics[\"Semantic Similarity\"] = None\n",
    "        if not sentence_model:\n",
    "            print(\"SentenceTransformer not loaded, cannot calculate Semantic Similarity.\")\n",
    "        elif not reference or not summary or summary.startswith(\"Error:\"):\n",
    "             print(\"Reference or summary is empty/errored, cannot calculate Semantic Similarity.\")\n",
    "\n",
    "\n",
    "    # Readability\n",
    "    if summary and not summary.startswith(\"Error:\"):\n",
    "        try:\n",
    "            metrics[\"Readability (Flesch)\"] = textstat.flesch_reading_ease(summary)\n",
    "        except Exception as e:\n",
    "             metrics[\"Readability (Flesch)\"] = None\n",
    "             print(f\"Error calculating readability: {e}\")\n",
    "    else:\n",
    "        metrics[\"Readability (Flesch)\"] = None\n",
    "        if not summary:\n",
    "             print(\"Summary is empty, cannot calculate readability.\")\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# --- Helper Function to get summary based on model key ---\n",
    "def get_summary(text, model_key):\n",
    "    \"\"\"Gets summary for a given text and model key.\"\"\"\n",
    "    summary = \"Error: Summarization failed.\" # Default error message\n",
    "    start_time = time.time()\n",
    "    proc_time = 0\n",
    "\n",
    "    try:\n",
    "        if model_key == 'textrank':\n",
    "            summary = textrank_summarize_summa(text)\n",
    "        elif model_key == 'bart' and bart_pipeline:\n",
    "             summary = bart_pipeline(text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "        elif model_key == 'tinyllama' and tinyllama_model_info:\n",
    "            tokenizer = tinyllama_model_info['tokenizer']\n",
    "            model = tinyllama_model_info['model']\n",
    "            chat = [{\"role\": \"user\", \"content\": f\"Summarize this text concisely:\\n\\n{text}\"}]\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "            summary = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "        elif model_key == 'phi' and phi_model_info:\n",
    "            tokenizer = phi_model_info['tokenizer']\n",
    "            model = phi_model_info['model']\n",
    "            prompt = f\"Instruct: Summarize the following text concisely.\\n{text}\\nOutput:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            summary = decoded.split(\"Output:\")[1].strip() if \"Output:\" in decoded else decoded\n",
    "        elif model_key == 'gemma' and gemma_model_info:\n",
    "            tokenizer = gemma_model_info['tokenizer']\n",
    "            model = gemma_model_info['model']\n",
    "            prompt = f\"Summarize the following text concisely and clearly:\\n\\n{text}\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "            summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            summary = f\"Error: Model '{model_key}' not available or not loaded.\"\n",
    "\n",
    "        proc_time = time.time() - start_time\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "         summary = f\"Error: CUDA Out of Memory for model '{model_key}'. Try a smaller model or a runtime with more GPU memory. Details: {e}\"\n",
    "    except Exception as e:\n",
    "        summary = f\"Error summarizing with model '{model_key}': {e}\"\n",
    "\n",
    "    return summary, proc_time\n",
    "\n",
    "\n",
    "# --- 4. GENERATE SUMMARIES AND EVALUATE FOR ALL SAMPLE TEXTS ---\n",
    "def evaluate_all_sample_texts():\n",
    "    all_results = []\n",
    "    # Use keys from MODELS or defined extractive keys for the loop\n",
    "    model_keys = [key for key in MODELS.keys() if key != 'embedding'] + ['textrank']\n",
    "\n",
    "\n",
    "    for file_name, original_text in sample_texts.items():\n",
    "        print(f\"--- Processing {file_name} ---\")\n",
    "        for model_key in model_keys:\n",
    "            # Determine the display name\n",
    "            model_name = model_key.capitalize() # Simple capitalization\n",
    "            # Check if the model key exists in the abstractive or extractive choices dictionary\n",
    "            # This is to get the user-friendly name like \"TinyLlama\" instead of \"tinyllama\"\n",
    "            found_name = False\n",
    "            for name, key in abstractive_model_choices.items():\n",
    "                if key == model_key:\n",
    "                    model_name = name\n",
    "                    found_name = True\n",
    "                    break\n",
    "            if not found_name:\n",
    "                for name, key in extractive_model_choices.items():\n",
    "                    if key == model_key:\n",
    "                        model_name = name\n",
    "                        found_name = True\n",
    "                        break\n",
    "            if not found_name:\n",
    "                model_name = model_key # Fallback to key if name not found in choices\n",
    "\n",
    "\n",
    "            print(f\"  Summarizing with {model_name}...\")\n",
    "            summary, proc_time = get_summary(original_text, model_key)\n",
    "\n",
    "            result_entry = {\n",
    "                \"Model\": model_name,\n",
    "                \"File\": file_name,\n",
    "                \"Processing Time (s)\": round(proc_time, 2) if proc_time is not None else None,\n",
    "                \"Error\": None, # Initialize Error column\n",
    "                \"ROUGE-1 F1\": None,\n",
    "                \"ROUGE-2 F1\": None,\n",
    "                \"ROUGE-L F1\": None,\n",
    "                \"Semantic Similarity\": None,\n",
    "                \"Readability (Flesch)\": None\n",
    "            }\n",
    "\n",
    "            if summary.startswith(\"Error:\"):\n",
    "                 print(f\"    {summary}\") # Print the error message\n",
    "                 result_entry[\"Error\"] = summary # Store the error message\n",
    "            else:\n",
    "                print(f\"    Summary: {summary[:100]}...\") # Print a snippet\n",
    "                metrics = calculate_metrics(original_text, summary)\n",
    "                result_entry.update(metrics)\n",
    "\n",
    "            all_results.append(result_entry)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Ensure all columns are present, even if some metrics were never calculated\n",
    "    df = pd.DataFrame(all_results)\n",
    "    metric_columns = [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\", \"Semantic Similarity\", \"Readability (Flesch)\"]\n",
    "    for col in metric_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None # Add missing metric columns as None\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- 5. RUN EVALUATION AND DISPLAY RESULTS ---\n",
    "print(\"Starting evaluation on sample texts...\")\n",
    "evaluation_df = evaluate_all_sample_texts()\n",
    "\n",
    "# --- 6. DISPLAY RESULTS ---\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "display(evaluation_df)\n",
    "\n",
    "# You can now use evaluation_df to create visualizations (bar charts, radar plots)\n",
    "# based on the average metrics per model or metrics per file.\n",
    "# Example: Calculating average metrics per model\n",
    "# Filter out rows with errors before calculating mean\n",
    "eval_df_no_errors = evaluation_df[evaluation_df['Error'].isna()].copy() # Filter where Error is NaN\n",
    "average_metrics = eval_df_no_errors.groupby(\"Model\").mean(numeric_only=True).reset_index()\n",
    "print(\"\\n--- Average Metrics per Model (Excluding Errors) ---\")\n",
    "display(average_metrics)\n",
    "\n",
    "# Example: Visualizing average metrics (you'll need to implement these plotting functions)\n",
    "# print(\"\\n--- Visualizing Average Metrics ---\")\n",
    "# display(create_bar_charts(average_metrics))\n",
    "# display(create_radar_chart(average_metrics)) # Radar chart might need specific metric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl4lc0XcItSn"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT301ldoI325"
   },
   "source": [
    "visualizing has Metrics Table,Bar Charts, Radar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b6cb608605dd4076b2ebf8045a188aed",
      "c3e92f70c55d4eab8c1f4d6c0b94eadc",
      "89fa2e4b937a422b93d605d4734ae683",
      "91be604079394aee8eb244dd91768af4",
      "3d023006032d4755a1f338e4d0379fe0",
      "5cf6238145d841079f8c972dd8527444",
      "c7756433e4754cca8cbeff4ffe269b77",
      "1a5a4af46fc843c0ac69930ae9d47b6e",
      "db49ded7f9d94ea4b026ed00a862856a",
      "72d2e4f0377547489e44fb0aa892ea44",
      "e405656c7d8848fd8b38e75156dbf267",
      "956f34bbb34a4d4ab12add9140cfe4ee",
      "71b2c1f271aa472588939599e4860599",
      "0a94a6fff54649699aeca1b07f7f42dc",
      "9dc0504add66416c944633216f750fbf",
      "30f5a5f746184fa284abfe7ff656daa0",
      "9cd3b8b366db4325a73b68a4c5472c3d",
      "47669a9666b9487095d620f9cf733255",
      "22e0ae8e6b624ad6bc6e5beafb0e026b",
      "6c4a30c9934046ed97e8e9ef4c13c9ee",
      "2164498459334d4d872da061c8f06d11",
      "5b1839feb849488bb5e285fd89a69e52",
      "df1e1fd895d64a47ab6f7f7c9b4fbdca",
      "5a93dd2719a445e9be68baac930c5450",
      "caa7fc868f06461f954e185b9682d57b",
      "7b47e2ce22df45d984d115d45a9495d5",
      "a341b487b6564711977a4d0106e593bb",
      "2b06f0ce6d9c499b975fd6c99cdb0520",
      "00870690bb5049dfa42898f98eaadf30",
      "e7582f59c78e4d3c85fc8fa33fcd8b1f",
      "8b3c5cc40fb04f349d9df0475b5c20e5",
      "9589bbbfdbd84cc895e613cf86548ee4",
      "aea805d5bc3e4925b4a0698749b1b703",
      "3393493efd80430994fa63684920b015",
      "e910fa6e041245c18764e81f7e4bf27e",
      "e5b08a6117754955a63b7a44b8aa96ad",
      "f689231609ad4ff0a15985933914b987",
      "c28c3087784d42578836f5d140fa4aca",
      "fc17fe8ca3dd47868e9a4d07a3c9024c",
      "f8b73ea5c93748b0a55abb3cad0f68ed",
      "27df11ef02164510ad69619a0b40279c",
      "06c40264e9744b8da0db4e68f3f96f94",
      "e828298f3752431f938a8e183e3408a8",
      "1a2adc7887e74771bc78eea430e35dc7",
      "269fb95566cd4ed5bfed1deb831c51bc",
      "5fcc43baeb7347ce8a890fdf3dbd75a4",
      "19b0bd88e0c7435895453195353714f9",
      "948bef1a8f4f40dca19b8aeaa0cacc3a",
      "754267f504784272829dc6fc20111353",
      "fa5eb598bed045e3af5a7b001778ea0b",
      "a11b745debd64609807809eea1ac6979",
      "9497e9b1c1c8412eb6232764e3f5f728",
      "ea0ad70095ce46e8ab57b130e1974110",
      "8beda593df3f463a946e64e63df59274"
     ]
    },
    "id": "e9oe0ZeqctQN",
    "outputId": "c53cff36-7791-4846-cb9d-92056a04c51c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cb608605dd4076b2ebf8045a188aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2> Section B: Summarize with Specific Models</h2><p>Paste text, choose summarizat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import pandas as pd, time\n",
    "\n",
    "# --- Model Choices ---\n",
    "abstractive_model_choices = {\n",
    "    \"TinyLlama\": \"tinyllama\",\n",
    "    \"Phi-2\": \"phi\",\n",
    "    \"BART\": \"bart\",\n",
    "    \"Gemma\": \"gemma\"\n",
    "}\n",
    "extractive_model_choices = {\n",
    "    \"TextRank\": \"textrank\"\n",
    "}\n",
    "\n",
    "metrics_history_specific = []\n",
    "\n",
    "# --- UI Widgets ---\n",
    "header_specific = widgets.HTML(\n",
    "    \"<h2> Section B: Summarize with Specific Models</h2>\"\n",
    "    \"<p>Paste text, choose summarization type, and select models to run.</p>\"\n",
    ")\n",
    "prompt_input_specific = widgets.Textarea(\n",
    "    placeholder=\"Paste your text here to summarize...\",\n",
    "    layout={\"height\": \"200px\", \"width\": \"99%\"}\n",
    ")\n",
    "summary_type_specific = widgets.RadioButtons(\n",
    "    options=[\"Abstractive\", \"Extractive\"], value=\"Abstractive\", description=\"Type:\"\n",
    ")\n",
    "model_checkboxes_out = widgets.Output()\n",
    "checkboxes = {}\n",
    "generate_button_specific = widgets.Button(\n",
    "    description=\"🚀 Generate Summaries\", button_style=\"primary\", icon=\"cogs\"\n",
    ")\n",
    "clear_button_specific = widgets.Button(\n",
    "    description=\"🧹 Clear Outputs\", button_style=\"warning\", icon=\"trash\"\n",
    ")\n",
    "\n",
    "# --- Outputs ---\n",
    "summary_output_specific = widgets.Output(\n",
    "    layout={\"height\": \"400px\", \"border\": \"1px solid #ccc\", \"padding\": \"10px\", \"overflow\": \"scroll\"}\n",
    ")\n",
    "metrics_table_output_specific = widgets.Output()\n",
    "bar_plot_output_specific = widgets.Output()\n",
    "radar_plot_output_specific = widgets.Output()\n",
    "\n",
    "output_accordion_specific = widgets.Accordion(\n",
    "    children=[metrics_table_output_specific, bar_plot_output_specific, radar_plot_output_specific]\n",
    ")\n",
    "output_accordion_specific.set_title(0, \"📊 Metrics Table\")\n",
    "output_accordion_specific.set_title(1, \"📈 Bar Charts\")\n",
    "output_accordion_specific.set_title(2, \"✨ Radar Plot\")\n",
    "\n",
    "# --- Helper: Update Checkboxes ---\n",
    "def update_checkboxes(s_type):\n",
    "    global checkboxes\n",
    "    checkboxes = {}\n",
    "    choices = abstractive_model_choices if s_type == \"Abstractive\" else extractive_model_choices\n",
    "    with model_checkboxes_out:\n",
    "        model_checkboxes_out.clear_output(wait=True)\n",
    "        for name, key in choices.items():\n",
    "            checkboxes[key] = widgets.Checkbox(value=True, description=name)\n",
    "        display(widgets.VBox(list(checkboxes.values())))\n",
    "\n",
    "# --- Summarization Functions ---\n",
    "def summarize_abstractive(text, model_key):\n",
    "    \"\"\"Summarizes text using an abstractive model.\"\"\"\n",
    "    model_info = MODELS.get(model_key)\n",
    "    if not model_info:\n",
    "        return f\"Error: Model '{model_key}' not found.\"\n",
    "\n",
    "    if model_key == 'bart':\n",
    "        # BART uses a pipeline directly\n",
    "        return model_info['summarizer'](text, max_length=120, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    else:\n",
    "        # Other models use tokenizer and model directly\n",
    "        tokenizer = model_info['tokenizer']\n",
    "        model = model_info['model']\n",
    "\n",
    "        if model_key == 'tinyllama':\n",
    "            chat = [{\"role\": \"user\", \"content\": f\"Summarize this text concisely:\\n\\n{text}\"}]\n",
    "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        elif model_key == 'phi':\n",
    "             prompt = f\"Instruct: Summarize the following text concisely.\\n{text}\\nOutput:\"\n",
    "        elif model_key == 'gemma':\n",
    "             prompt = f\"Summarize the following text concisely and clearly:\\n\\n{text}\"\n",
    "        else:\n",
    "            prompt = f\"Summarize the following text:\\n\\n{text}\" # Default prompt\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Post-processing for specific models if needed\n",
    "        if model_key == 'phi' and \"Output:\" in decoded:\n",
    "             decoded = decoded.split(\"Output:\")[1].strip()\n",
    "        elif model_key == 'tinyllama':\n",
    "             # Assuming the response starts after the prompt for TinyLlama\n",
    "             decoded = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        return decoded\n",
    "\n",
    "def summarize_extractive(text, model_key):\n",
    "    \"\"\"Summarizes text using an extractive model.\"\"\"\n",
    "    if model_key == \"textrank\":\n",
    "        # Use the imported textrank_summarizer\n",
    "        # You might want to adjust the ratio or number of sentences here\n",
    "        return textrank_summarizer.summarize(text, ratio=0.3) # Adjust ratio as needed\n",
    "    else:\n",
    "        return f\"Error: Extractive model '{model_key}' not supported.\"\n",
    "\n",
    "# --- Metrics Calculation (Assuming calculate_metrics, create_bar_charts, create_radar_chart are defined elsewhere) ---\n",
    "# Placeholder functions - replace with your actual implementations\n",
    "def calculate_metrics(summary, original_text, proc_time):\n",
    "    \"\"\"Calculates ROUGE, Semantic Similarity, Readability, and Compression.\"\"\"\n",
    "    # This is a placeholder. Replace with the actual metric calculation logic.\n",
    "    # You'll need to import rouge_scorer, SentenceTransformer, textstat, etc.\n",
    "    try:\n",
    "        readability = textstat.flesch_reading_ease(summary)\n",
    "    except:\n",
    "        readability = 0 # Handle potential errors with textstat\n",
    "\n",
    "    try:\n",
    "        compression = round((1 - len(summary) / len(original_text)) * 100, 2) if len(original_text) > 0 else 0\n",
    "    except:\n",
    "        compression = 0 # Handle potential division by zero\n",
    "\n",
    "    # Placeholder for other metrics\n",
    "    return {\n",
    "        \"Readability\": readability,\n",
    "        \"Compression (%)\": compression,\n",
    "        \"Processing Time (s)\": round(proc_time, 2),\n",
    "        # Add placeholders for ROUGE and Semantic Similarity if you want to include them\n",
    "        \"ROUGE-1 F1\": 0,\n",
    "        \"Semantic Similarity\": 0,\n",
    "    }\n",
    "\n",
    "def create_bar_charts(df):\n",
    "    \"\"\"Generates bar charts for metrics.\"\"\"\n",
    "    # This is a placeholder. Replace with your actual bar chart generation logic.\n",
    "    # You'll need matplotlib.pyplot and seaborn\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import io\n",
    "\n",
    "    if df.empty:\n",
    "        return HTML(\"<p>No data to plot yet.</p>\")\n",
    "\n",
    "    metrics_to_plot = [\"Readability\", \"Compression (%)\", \"Processing Time (s)\", \"ROUGE-1 F1\", \"Semantic Similarity\"]\n",
    "    plot_html = \"\"\n",
    "\n",
    "    for metric in metrics_to_plot:\n",
    "        if metric in df.columns:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.barplot(x=\"Model\", y=metric, data=df)\n",
    "            plt.title(f\"{metric} by Model\")\n",
    "            plt.ylabel(metric)\n",
    "            plt.xlabel(\"Model\")\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            img_base64 = base66.b64encode(buf.read()).decode('utf-8')\n",
    "            plt.close()\n",
    "            plot_html += f'<img src=\"data:image/png;base64,{img_base64}\"/><br>'\n",
    "\n",
    "    return HTML(plot_html)\n",
    "\n",
    "\n",
    "def create_radar_chart(df):\n",
    "    \"\"\"Generates a radar chart for normalized metrics.\"\"\"\n",
    "    # This is a placeholder. Replace with your actual radar chart generation logic.\n",
    "    # You'll need matplotlib.pyplot and numpy\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import io\n",
    "    import base64\n",
    "\n",
    "    if df.empty:\n",
    "        return HTML(\"<p>No data to plot yet.</p>\")\n",
    "\n",
    "    # Select metrics for the radar chart (choose metrics that make sense for comparison)\n",
    "    radar_metrics = [\"Readability\", \"Compression (%)\", \"Processing Time (s)\", \"ROUGE-1 F1\", \"Semantic Similarity\"]\n",
    "\n",
    "    # Normalize data (simple min-max scaling)\n",
    "    df_normalized = df[radar_metrics].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0)\n",
    "    df_normalized['Model'] = df['Model']\n",
    "\n",
    "    categories = radar_metrics\n",
    "    N = len(categories)\n",
    "\n",
    "    # What angle each axis will start at\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1] # Complete the circle\n",
    "\n",
    "    plot_html = \"\"\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # Draw one axe per instance and add labels\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=10)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75, 1.0], [\"0.25\", \"0.50\", \"0.75\", \"1.00\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Plot data and add a line plot per model\n",
    "    for index, row in df_normalized.iterrows():\n",
    "        values = row[categories].tolist()\n",
    "        values += values[:1] # Complete the circle\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=row['Model'])\n",
    "        ax.fill(angles, values, alpha=0.4) # Fill area\n",
    "\n",
    "    plt.title(\"Normalized Metrics Comparison\", size=12, color='grey', y=1.1)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    plt.close()\n",
    "    plot_html += f'<img src=\"data:image/png;base64,{img_base64}\"/>'\n",
    "\n",
    "    return HTML(plot_html)\n",
    "\n",
    "\n",
    "# --- Event: Generate Summaries ---\n",
    "def on_generate_button_clicked_specific(b):\n",
    "    generate_button_specific.disabled = True\n",
    "    generate_button_specific.description = \"Processing...\"\n",
    "    original_text = prompt_input_specific.value.strip()\n",
    "    s_type = summary_type_specific.value\n",
    "    s_keys = [key for key, cb in checkboxes.items() if cb.value]\n",
    "\n",
    "    if not original_text or not s_keys:\n",
    "        with summary_output_specific:\n",
    "            clear_output()\n",
    "            print(\"⚠️ Please enter text and select at least one model.\")\n",
    "        generate_button_specific.disabled = False\n",
    "        generate_button_specific.description = \"🚀 Generate Summaries\"\n",
    "        return\n",
    "\n",
    "    with summary_output_specific:\n",
    "        clear_output()\n",
    "        display(HTML(f\"<hr><h2>Processing Pasted Text ({s_type})</h2>\"))\n",
    "\n",
    "    current_metrics = [] # Store metrics for this run\n",
    "\n",
    "    for model_key in s_keys:\n",
    "        model_name = (\n",
    "            [k for k, v in abstractive_model_choices.items() if v == model_key]\n",
    "            + [k for k, v in extractive_model_choices.items() if v == model_key]\n",
    "        )[0]\n",
    "\n",
    "        with summary_output_specific:\n",
    "            print(f\"⏳ Summarizing with {model_name}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            summary = (\n",
    "                summarize_abstractive(original_text, model_key)\n",
    "                if s_type == \"Abstractive\"\n",
    "                else summarize_extractive(original_text, model_key)\n",
    "            )\n",
    "            proc_time = time.time() - start_time\n",
    "\n",
    "            with summary_output_specific:\n",
    "                display(HTML(f\"<h3>Summary from <b>{model_name}</b></h3><p>{summary}</p>\"))\n",
    "\n",
    "            metrics = calculate_metrics(summary, original_text, proc_time)\n",
    "            metrics.update({\"Model\": model_name, \"File\": \"Pasted Text\", \"Type\": s_type})\n",
    "            current_metrics.append(metrics) # Add to current run's metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            with summary_output_specific:\n",
    "                print(f\"❌ Error summarizing with {model_name}: {e}\")\n",
    "            # Optionally add an entry for the failed model with error info\n",
    "            metrics = {\"Model\": model_name, \"File\": \"Pasted Text\", \"Type\": s_type, \"Error\": str(e)}\n",
    "            current_metrics.append(metrics) # Add error entry\n",
    "\n",
    "    # Append current run's metrics to history\n",
    "    metrics_history_specific.extend(current_metrics)\n",
    "\n",
    "\n",
    "    # --- Display Metrics ---\n",
    "    if current_metrics: # Only update if there were successful runs\n",
    "        df = pd.DataFrame(metrics_history_specific) # Use the full history\n",
    "        with metrics_table_output_specific:\n",
    "            metrics_table_output_specific.clear_output(wait=True)\n",
    "            display(df)\n",
    "\n",
    "        with bar_plot_output_specific:\n",
    "            bar_plot_output_specific.clear_output(wait=True)\n",
    "            display(create_bar_charts(df))\n",
    "\n",
    "        with radar_plot_output_specific:\n",
    "            radar_plot_output_specific.clear_output(wait=True)\n",
    "            display(create_radar_chart(df))\n",
    "    else:\n",
    "         with metrics_table_output_specific:\n",
    "            metrics_table_output_specific.clear_output(wait=True)\n",
    "            print(\"No successful summaries to display metrics.\")\n",
    "         with bar_plot_output_specific:\n",
    "            bar_plot_output_specific.clear_output(wait=True)\n",
    "         with radar_plot_output_specific:\n",
    "            radar_plot_output_specific.clear_output(wait=True)\n",
    "\n",
    "\n",
    "    generate_button_specific.disabled = False\n",
    "    generate_button_specific.description = \"🚀 Generate Summaries\"\n",
    "\n",
    "# --- Event: Clear All ---\n",
    "def on_clear_button_clicked_specific(b):\n",
    "    global metrics_history_specific\n",
    "    metrics_history_specific = []\n",
    "    summary_output_specific.clear_output()\n",
    "    metrics_table_output_specific.clear_output()\n",
    "    bar_plot_output_specific.clear_output()\n",
    "    radar_plot_output_specific.clear_output()\n",
    "    prompt_input_specific.value = \"\"\n",
    "    with summary_output_specific:\n",
    "        print(\"🧹 All outputs cleared.\")\n",
    "\n",
    "# --- Event: Summary Type Change ---\n",
    "def on_summary_type_change_specific(change):\n",
    "    update_checkboxes(change.new)\n",
    "\n",
    "generate_button_specific.on_click(on_generate_button_clicked_specific)\n",
    "clear_button_specific.on_click(on_clear_button_clicked_specific)\n",
    "summary_type_specific.observe(on_summary_type_change_specific, names=\"value\")\n",
    "\n",
    "# --- Assemble and Display UI ---\n",
    "input_controls_specific = widgets.VBox(\n",
    "    [\n",
    "        prompt_input_specific,\n",
    "        summary_type_specific,\n",
    "        widgets.Label(\"Select Models:\"),\n",
    "        model_checkboxes_out,\n",
    "        widgets.HBox([generate_button_specific, clear_button_specific]),\n",
    "    ],\n",
    "    layout=widgets.Layout(width=\"35%\", padding=\"10px\", border=\"1px solid lightgrey\", border_radius=\"5px\"),\n",
    ")\n",
    "\n",
    "output_area_specific = widgets.VBox(\n",
    "    [summary_output_specific, output_accordion_specific],\n",
    "    layout=widgets.Layout(width=\"65%\", padding=\"10px\"),\n",
    ")\n",
    "\n",
    "app_specific = widgets.VBox(\n",
    "    [header_specific, widgets.HBox([input_controls_specific, output_area_specific])]\n",
    ")\n",
    "\n",
    "# --- Initialize ---\n",
    "update_checkboxes(\"Abstractive\")\n",
    "display(app_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTBr_aljHpOv"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
